{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.DataFrame\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "/** Calculates the homogeneity custer scoring \n",
    " *\n",
    " *  @param df a Spark DataFrame of the input data\n",
    " *  @param label a String column name of a column in the df where the true class is stored\n",
    " *  @param cluster a String column name of a column in the df where the cluster id is stored\n",
    " */\n",
    "def homogeneity_score(df: DataFrame, label: String, cluster: String): Double = {\n",
    "  def log2 = (x: Double) => scala.math.log10(x)/scala.math.log10(2.0)\n",
    "\n",
    "  def entropy(count: Int, n: Long): Double ={\n",
    "    -(count.toDouble / n) * log2(count.toDouble / n)\n",
    "  }\n",
    "\n",
    "  def c_entropy(count: Int, n: Long, k: Long): Double ={\n",
    "    -(count.toDouble / n) * log2(count.toDouble / k)\n",
    "  }\n",
    "\n",
    "  val udf_entropy = udf(entropy _)\n",
    "  val udf_c_entropy = udf(c_entropy _)\n",
    "\n",
    "\n",
    "  //filtering for two or more counts to \n",
    "  //var df_two_plus = (df.groupBy(label)\n",
    "  //                     .agg(count(lit(1)).alias(\"count\"))\n",
    "  //                     .as(\"df1\")\n",
    "  //                     .join(df.as(\"df2\"), label)\n",
    "  //                     .filter(\"count > 1\")).select(label,cluster)\n",
    "\n",
    "  val n = df.count().toLong\n",
    "  val classes = df.groupBy(label).count()\n",
    "  val clusters = df.groupBy(cluster).count().toDF(cluster, \"count_k\")\n",
    "  // number of class c assigned to cluster k\n",
    "  val n_ck = df.groupBy(label,cluster).count()\n",
    "\n",
    "  val entropy_of_classes = (classes.withColumn(\"entropy\", udf_entropy(classes(\"count\"), lit(n)))\n",
    "                                   .agg(sum(\"entropy\"))\n",
    "                                   .first()\n",
    "                                   .getDouble(0))\n",
    "  \n",
    "  val joined_df = n_ck.as(\"n_ck\").join(clusters, cluster)\n",
    "  val conditional_entropy = (joined_df.withColumn(\"c_entropy\", udf_c_entropy(joined_df(\"count\"), lit(n), joined_df(\"count_k\")))\n",
    "                                 .agg(sum(\"c_entropy\"))\n",
    "                                 .first()\n",
    "                                 .getDouble(0))\n",
    "    \n",
    "  1 - conditional_entropy.toDouble / entropy_of_classes\n",
    "}\n",
    "\n",
    "/** Calculates the completeness custer scoring \n",
    " *\n",
    " *  @param df a Spark DataFrame of the input data\n",
    " *  @param label a String column name of a column in the df where the true class is stored\n",
    " *  @param cluster a String column name of a column in the df where the cluster id is stored\n",
    " */\n",
    "def completeness_score(df: DataFrame, label: String, cluster: String): Double = {\n",
    "  homogeneity_score(df, cluster, label)\n",
    "}\n",
    "\n",
    "/** Calculates the harmonic mean / v measurement of the custer scoring \n",
    " *\n",
    " *  @param df a Spark DataFrame of the input data\n",
    " *  @param label a String column name of a column in the df where the true class is stored\n",
    " *  @param cluster a String column name of a column in the df where the cluster id is stored\n",
    " */\n",
    "def v_measurement_score(df: DataFrame, label: String, cluster: String): Double = {\n",
    "  val h = homogeneity_score(df, label, cluster)\n",
    "  val c = completeness_score(df, label, cluster)\n",
    "  2 * h * c / (h + c)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// Dataset 1\n",
    "case class jz_row(label: String, cluster: String)\n",
    "val table = Seq(jz_row(\"0\", \"a\"),jz_row(\"0\", \"a\"),jz_row(\"0\", \"a\"),jz_row(\"0\", \"b\"),jz_row(\"1\", \"b\"),jz_row(\"1\", \"c\"),jz_row(\"1\", \"c\"),jz_row(\"2\",\"d\"))\n",
    "var df = spark.createDataFrame(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// Dataset 2\n",
    "val schema = df.schema\n",
    "val labels_true = List(\"0,0,0,1,1,1,3,3,3,5,5,5,5,5,5,5,5\").flatMap(_.split(\",\"))\n",
    "val labels_pred = List(\"0,1,1,1,1,1,3,3,3,5,5,5,5,5,5,5,5\").flatMap(_.split(\",\"))\n",
    "val rows = labels_pred zip labels_true\n",
    "val rdd = sc.parallelize (rows).map(x => Row(x._1, x._2))\n",
    "df = spark.sqlContext.createDataFrame(rdd, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// Dataset 2\n",
    "val schema = df.schema\n",
    "val labels_true = List(\"0,0,0,1,1,1,3,3,3,5,5,5,5,5,5,5,5\").flatMap(_.split(\",\"))\n",
    "val labels_pred = List(\"0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\").flatMap(_.split(\",\"))\n",
    "val rows = labels_true zip labels_pred\n",
    "val rdd = sc.parallelize (rows).map(x => Row(x._1, x._2))\n",
    "df = spark.sqlContext.createDataFrame(rdd, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+\n",
      "|label|cluster|\n",
      "+-----+-------+\n",
      "|    0|      0|\n",
      "|    0|      0|\n",
      "|    0|      0|\n",
      "|    1|      0|\n",
      "|    1|      0|\n",
      "|    1|      0|\n",
      "|    3|      0|\n",
      "|    3|      0|\n",
      "|    3|      0|\n",
      "|    5|      0|\n",
      "|    5|      0|\n",
      "|    5|      0|\n",
      "|    5|      0|\n",
      "|    5|      0|\n",
      "|    5|      0|\n",
      "|    5|      0|\n",
      "|    5|      0|\n",
      "+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completeness_score(df, \"label\", \"cluster\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NaN"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "homogeneity_score(df, \"label\", \"cluster\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "file_extension": ".scala",
   "name": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
