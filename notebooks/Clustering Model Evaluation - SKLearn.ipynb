{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custering Model Evaluation\n",
    "### For reference see Scikit Learn's [clustering and model evaluation](http://scikit-learn.org/stable/modules/clustering.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.DataFrame\n",
    "\n",
    "/** Calculates the homogeneity custer scoring \n",
    " *\n",
    " *  @param df a Spark DataFrame of the input data\n",
    " *  @param truth a String column name of a column in the df where the true class is stored\n",
    " *  @param pred a String column name of a column in the df where the cluster id is stored\n",
    " */\n",
    "def homogeneity_score(df: DataFrame, truth: String, pred: String): Double = {\n",
    "  import org.apache.spark.sql.functions.{sum, udf, lit, count}\n",
    "    \n",
    "  def log2 = (x: Double) => scala.math.log10(x)/scala.math.log10(2.0)\n",
    "\n",
    "  def entropy(count: Int, n: Long, n_k: Long): Double = {\n",
    "    -(count.toDouble / n) * log2(count.toDouble / n_k)\n",
    "  }\n",
    "  val udf_entropy = udf(entropy _)\n",
    "\n",
    "  val n = df.count().toLong\n",
    "  val classes = df.groupBy(truth).count()\n",
    "  val clusters = df.groupBy(pred).count().toDF(pred, \"count_k\")\n",
    "  // number of class c assigned to cluster k\n",
    "  val n_ck = df.groupBy(truth,pred).count()\n",
    "\n",
    "  val entropy_of_classes = (classes.withColumn(\"entropy\", \n",
    "                                               udf_entropy(classes(\"count\"),\n",
    "                                                           lit(n), \n",
    "                                                           lit(n)))\n",
    "                                   .agg(sum(\"entropy\"))\n",
    "                                   .first()\n",
    "                                   .getDouble(0))\n",
    "  \n",
    "  val joined_df = n_ck.as(\"n_ck\").join(clusters, pred)\n",
    "  val conditional_entropy = (joined_df.withColumn(\"c_entropy\", \n",
    "                                                  udf_entropy(joined_df(\"count\"), \n",
    "                                                                lit(n), \n",
    "                                                                joined_df(\"count_k\")))\n",
    "                                      .agg(sum(\"c_entropy\"))\n",
    "                                      .first()\n",
    "                                      .getDouble(0))\n",
    "    \n",
    "  1 - conditional_entropy.toDouble / entropy_of_classes\n",
    "}\n",
    "\n",
    "/** Calculates the completeness custer scoring \n",
    " *\n",
    " *  @param df a Spark DataFrame of the input data\n",
    " *  @param truth a String column name of a column in the df where the true class is stored\n",
    " *  @param pred a String column name of a column in the df where the cluster id is stored\n",
    " *  @param two_plus Boolean to state whether or not to filter for only two or greater count in the truth values\n",
    " */\n",
    "def completeness_score(df: DataFrame, truth: String, pred: String, two_plus: Boolean = false): Double = {\n",
    "  import org.apache.spark.sql.functions.{sum, udf, lit, count}\n",
    "  var filtered_df = df\n",
    "  if (two_plus){\n",
    "    println(\"filtering for 2+\")\n",
    "    filtered_df = (df.groupBy(truth)\n",
    "                     .agg(count(lit(1)).alias(\"count\"))\n",
    "                     .as(\"df1\")\n",
    "                     .join(df.as(\"df2\"), truth)\n",
    "                     .filter(\"count > 1\").select(truth, pred))\n",
    "  }\n",
    "  homogeneity_score(filtered_df, pred, truth)\n",
    "}\n",
    "\n",
    "/** Calculates the harmonic mean / v measurement of the custer scoring \n",
    " *\n",
    " *  @param df a Spark DataFrame of the input data\n",
    " *  @param truth a String column name of a column in the df where the true class is stored\n",
    " *  @param pred a String column name of a column in the df where the cluster id is stored\n",
    " */\n",
    "def v_measurement_score(df: DataFrame, truth: String, pred: String): Double = {\n",
    "  val h = homogeneity_score(df, truth, pred)\n",
    "  val c = completeness_score(df, truth, pred)\n",
    "  2 * h * c / (h + c)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.Row\n",
    "\n",
    "// Dataset 1\n",
    "case class jz_row(truth: String, pred: String)\n",
    "val table = Seq(jz_row(\"0\", \"a\"),jz_row(\"0\", \"a\"),jz_row(\"0\", \"a\"),jz_row(\"0\", \"b\"),jz_row(\"1\", \"b\"),jz_row(\"1\", \"c\"),jz_row(\"1\", \"c\"),jz_row(\"2\",\"d\"))\n",
    "var df = spark.createDataFrame(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// Dataset 2\n",
    "val schema = df.schema\n",
    "val labels_true = List(\"0,0,0,1,1,1,3,3,3,5,5,5,5,5,5,5,5\").flatMap(_.split(\",\"))\n",
    "val labels_pred = List(\"0,1,1,1,1,1,3,3,3,5,5,5,5,5,5,5,5\").flatMap(_.split(\",\"))\n",
    "val rows = labels_true zip labels_pred\n",
    "val rdd = sc.parallelize (rows).map(x => Row(x._1, x._2))\n",
    "df = spark.sqlContext.createDataFrame(rdd, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// Dataset 3\n",
    "val schema = df.schema\n",
    "val labels_true = List(\"0,0,0,1,1,1,3,3,3,5,5,5,5,5,5,5,5,6,7,8\").flatMap(_.split(\",\"))\n",
    "val labels_pred = List(\"0,1,1,1,1,1,3,3,3,5,5,5,5,5,5,5,5,7,8,9\").flatMap(_.split(\",\"))\n",
    "\n",
    "val rows = labels_true zip labels_pred\n",
    "val rdd = sc.parallelize (rows).map(x => Row(x._1, x._2))\n",
    "df = spark.sqlContext.createDataFrame(rdd, schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+\n",
      "|truth|pred|\n",
      "+-----+----+\n",
      "|    0|   0|\n",
      "|    0|   1|\n",
      "|    0|   1|\n",
      "|    1|   1|\n",
      "|    1|   1|\n",
      "|    1|   1|\n",
      "|    3|   3|\n",
      "|    3|   3|\n",
      "|    3|   3|\n",
      "|    5|   5|\n",
      "|    5|   5|\n",
      "|    5|   5|\n",
      "|    5|   5|\n",
      "|    5|   5|\n",
      "|    5|   5|\n",
      "|    5|   5|\n",
      "|    5|   5|\n",
      "|    6|   7|\n",
      "|    7|   8|\n",
      "|    8|   9|\n",
      "+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8992244133520476"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "homogeneity_score(df, \"truth\", \"pred\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.940207373487672"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completeness_score(df, \"truth\", \"pred\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtering for 2+\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9054029493697273"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completeness_score(df, \"truth\", \"pred\", true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9192593385659383"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_measurement_score(df, \"truth\", \"pred\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "file_extension": ".scala",
   "name": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
